"""
Training pipeline for SignalMAE on RF spectrograms.

Provides a simple training loop for pre-training SignalMAE on
TorchSig-generated RF spectrograms and fine-tuning for classification.
"""

import logging
import time
from pathlib import Path
from typing import Optional, Dict, Any, Tuple

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from src.config import Config, create_rf_config

logger = logging.getLogger(__name__)


class RFSpectrogramDataset(Dataset):
    """
    Dataset for RF spectrograms generated by TorchSig.

    Loads precomputed spectrograms from .npy files for pre-training
    or classification.

    Args:
        data_dir: Directory containing spectrogram .npy files
        metadata_csv: Path to metadata CSV with labels
        transform: Optional transform to apply
        return_labels: Whether to return labels (for classification)
    """

    def __init__(
        self,
        data_dir: Path,
        metadata_csv: Optional[Path] = None,
        transform: Optional[Any] = None,
        return_labels: bool = False,
    ):
        self.data_dir = Path(data_dir)
        self.transform = transform
        self.return_labels = return_labels

        # Find all spectrogram files
        self.files = sorted(self.data_dir.glob("*.npy"))

        if len(self.files) == 0:
            raise ValueError(f"No .npy files found in {data_dir}")

        # Load metadata if provided
        self.labels = None
        self.label_to_idx = None
        if metadata_csv is not None and return_labels:
            import pandas as pd
            df = pd.read_csv(metadata_csv)

            # Build label mapping
            unique_labels = sorted(df['label'].unique())
            self.label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}

            # Map filenames to labels
            self.labels = {}
            for _, row in df.iterrows():
                filename = row['filename']
                label = row['label']
                self.labels[filename] = self.label_to_idx[label]

        logger.info(f"Loaded {len(self.files)} spectrograms from {data_dir}")

    def __len__(self) -> int:
        return len(self.files)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:
        # Load spectrogram
        spec_path = self.files[idx]
        spectrogram = np.load(spec_path)

        # Convert to tensor [H, W] -> [1, H, W] -> [3, H, W]
        if spectrogram.ndim == 2:
            spectrogram = np.stack([spectrogram] * 3, axis=0)
        elif spectrogram.ndim == 3 and spectrogram.shape[0] == 1:
            spectrogram = np.concatenate([spectrogram] * 3, axis=0)

        tensor = torch.from_numpy(spectrogram).float()

        # Apply transform if provided
        if self.transform is not None:
            tensor = self.transform(tensor)

        # Return with label if classification
        if self.return_labels and self.labels is not None:
            filename = spec_path.name
            label = self.labels.get(filename, 0)
            return tensor, label

        return (tensor,)

    @property
    def num_classes(self) -> int:
        """Number of unique classes."""
        if self.label_to_idx is not None:
            return len(self.label_to_idx)
        return 0


class SignalMAETrainer:
    """
    Trainer for SignalMAE pre-training and fine-tuning.

    Provides a simple training loop with:
    - Learning rate warmup
    - Gradient clipping
    - Checkpointing
    - Progress logging

    Example:
        from src.models.signalmae import SignalMAE
        from src.training.rf_trainer import SignalMAETrainer
        from src.config import create_rf_config

        config = create_rf_config("small")
        model = SignalMAE(config)

        trainer = SignalMAETrainer(
            model=model,
            config=config,
            device="cuda",
        )

        trainer.pretrain(
            train_loader=train_loader,
            val_loader=val_loader,
            output_dir="checkpoints/signalmae",
        )

    Args:
        model: SignalMAE model to train
        config: Training configuration
        device: Device to train on ("cuda" or "cpu")
    """

    def __init__(
        self,
        model: nn.Module,
        config: Config,
        device: str = "cuda",
    ):
        self.model = model.to(device)
        self.config = config
        self.device = device

        # Create optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.95),
        )

        # Gradient clipping
        self.max_grad_norm = 1.0

    def _get_lr(self, epoch: int) -> float:
        """Get learning rate with warmup and cosine decay."""
        if epoch < self.config.warmup_epochs:
            # Linear warmup
            return self.config.learning_rate * (epoch + 1) / self.config.warmup_epochs
        else:
            # Cosine decay
            progress = (epoch - self.config.warmup_epochs) / (
                self.config.epochs - self.config.warmup_epochs
            )
            return self.config.learning_rate * 0.5 * (1 + np.cos(np.pi * progress))

    def _set_lr(self, lr: float) -> None:
        """Set learning rate for optimizer."""
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

    def pretrain_epoch(
        self,
        train_loader: DataLoader,
        epoch: int,
    ) -> Dict[str, float]:
        """
        Run one pre-training epoch.

        Args:
            train_loader: DataLoader for training data
            epoch: Current epoch number

        Returns:
            Dictionary with loss metrics
        """
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        # Set learning rate
        lr = self._get_lr(epoch)
        self._set_lr(lr)

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

        for batch in pbar:
            # Handle batch format
            if isinstance(batch, (list, tuple)):
                spectrograms = batch[0].to(self.device)
            else:
                spectrograms = batch.to(self.device)

            # Forward pass with masking
            self.optimizer.zero_grad()
            loss, pred, mask = self.model(
                spectrograms,
                mask_ratio=self.config.mask_ratio,
            )

            # Handle multi-loss output
            if isinstance(loss, tuple):
                loss = loss[0]  # Total loss

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.max_grad_norm,
            )
            self.optimizer.step()

            # Track metrics
            total_loss += loss.item()
            num_batches += 1

            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'lr': f"{lr:.2e}",
            })

        avg_loss = total_loss / num_batches
        return {'loss': avg_loss, 'lr': lr}

    @torch.no_grad()
    def validate(
        self,
        val_loader: DataLoader,
    ) -> Dict[str, float]:
        """
        Run validation.

        Args:
            val_loader: DataLoader for validation data

        Returns:
            Dictionary with validation metrics
        """
        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        for batch in val_loader:
            if isinstance(batch, (list, tuple)):
                spectrograms = batch[0].to(self.device)
            else:
                spectrograms = batch.to(self.device)

            loss, _, _ = self.model(
                spectrograms,
                mask_ratio=self.config.mask_ratio,
            )

            if isinstance(loss, tuple):
                loss = loss[0]

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches
        return {'val_loss': avg_loss}

    def pretrain(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        output_dir: Optional[Path] = None,
    ) -> Dict[str, Any]:
        """
        Run full pre-training loop.

        Args:
            train_loader: DataLoader for training data
            val_loader: Optional DataLoader for validation
            output_dir: Directory to save checkpoints

        Returns:
            Training history dictionary
        """
        if output_dir is not None:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

        history = {
            'train_loss': [],
            'val_loss': [],
            'lr': [],
        }

        best_val_loss = float('inf')
        start_time = time.time()

        for epoch in range(self.config.epochs):
            # Train
            train_metrics = self.pretrain_epoch(train_loader, epoch)
            history['train_loss'].append(train_metrics['loss'])
            history['lr'].append(train_metrics['lr'])

            # Validate
            if val_loader is not None:
                val_metrics = self.validate(val_loader)
                history['val_loss'].append(val_metrics['val_loss'])

                logger.info(
                    f"Epoch {epoch+1}/{self.config.epochs}: "
                    f"train_loss={train_metrics['loss']:.4f}, "
                    f"val_loss={val_metrics['val_loss']:.4f}"
                )

                # Save best model
                if output_dir is not None and val_metrics['val_loss'] < best_val_loss:
                    best_val_loss = val_metrics['val_loss']
                    torch.save(
                        self.model.state_dict(),
                        output_dir / "best_model.pt",
                    )
            else:
                logger.info(
                    f"Epoch {epoch+1}/{self.config.epochs}: "
                    f"train_loss={train_metrics['loss']:.4f}"
                )

            # Save checkpoint
            if output_dir is not None and (epoch + 1) % 10 == 0:
                torch.save(
                    {
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'config': self.config.to_dict(),
                    },
                    output_dir / f"checkpoint_epoch{epoch+1}.pt",
                )

        # Save final model
        if output_dir is not None:
            torch.save(self.model.state_dict(), output_dir / "final_model.pt")

        total_time = time.time() - start_time
        logger.info(f"Training completed in {total_time/60:.1f} minutes")

        return history


class SignalMAEClassifierTrainer(SignalMAETrainer):
    """
    Trainer for SignalMAE classification fine-tuning.

    Extends SignalMAETrainer with classification-specific training.

    Args:
        model: SignalMAEClassifier model
        config: Training configuration
        device: Device to train on
    """

    def __init__(
        self,
        model: nn.Module,
        config: Config,
        device: str = "cuda",
    ):
        super().__init__(model, config, device)
        self.criterion = nn.CrossEntropyLoss()

    def finetune_epoch(
        self,
        train_loader: DataLoader,
        epoch: int,
    ) -> Dict[str, float]:
        """
        Run one fine-tuning epoch.

        Args:
            train_loader: DataLoader with (spectrogram, label) pairs
            epoch: Current epoch number

        Returns:
            Dictionary with loss and accuracy metrics
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        lr = self._get_lr(epoch)
        self._set_lr(lr)

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

        for spectrograms, labels in pbar:
            spectrograms = spectrograms.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()
            logits = self.model(spectrograms)
            loss = self.criterion(logits, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.max_grad_norm,
            )
            self.optimizer.step()

            # Track metrics
            total_loss += loss.item()
            _, predicted = logits.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'acc': f"{100*correct/total:.1f}%",
            })

        return {
            'loss': total_loss / len(train_loader),
            'accuracy': correct / total,
            'lr': lr,
        }

    @torch.no_grad()
    def validate_classification(
        self,
        val_loader: DataLoader,
    ) -> Dict[str, float]:
        """
        Run classification validation.

        Args:
            val_loader: DataLoader with (spectrogram, label) pairs

        Returns:
            Dictionary with validation metrics
        """
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        for spectrograms, labels in val_loader:
            spectrograms = spectrograms.to(self.device)
            labels = labels.to(self.device)

            logits = self.model(spectrograms)
            loss = self.criterion(logits, labels)

            total_loss += loss.item()
            _, predicted = logits.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

        return {
            'val_loss': total_loss / len(val_loader),
            'val_accuracy': correct / total,
        }

    def finetune(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        output_dir: Optional[Path] = None,
    ) -> Dict[str, Any]:
        """
        Run full fine-tuning loop.

        Args:
            train_loader: DataLoader for training
            val_loader: Optional DataLoader for validation
            output_dir: Directory to save checkpoints

        Returns:
            Training history
        """
        if output_dir is not None:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

        history = {
            'train_loss': [],
            'train_accuracy': [],
            'val_loss': [],
            'val_accuracy': [],
        }

        best_val_acc = 0.0

        for epoch in range(self.config.epochs):
            # Train
            train_metrics = self.finetune_epoch(train_loader, epoch)
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])

            # Validate
            if val_loader is not None:
                val_metrics = self.validate_classification(val_loader)
                history['val_loss'].append(val_metrics['val_loss'])
                history['val_accuracy'].append(val_metrics['val_accuracy'])

                logger.info(
                    f"Epoch {epoch+1}: "
                    f"train_acc={train_metrics['accuracy']*100:.1f}%, "
                    f"val_acc={val_metrics['val_accuracy']*100:.1f}%"
                )

                # Save best model
                if output_dir is not None and val_metrics['val_accuracy'] > best_val_acc:
                    best_val_acc = val_metrics['val_accuracy']
                    torch.save(
                        self.model.state_dict(),
                        output_dir / "best_classifier.pt",
                    )

        if output_dir is not None:
            torch.save(self.model.state_dict(), output_dir / "final_classifier.pt")

        return history
