{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Audio Embedding Visualization System\n",
    "\n",
    "This notebook generates AudioMAE embeddings from audio data and visualizes them in FiftyOne with:\n",
    "- Similarity search\n",
    "- Geographic visualization (lat/long mapping)\n",
    "- UMAP/t-SNE embedding space visualization\n",
    "- Metadata filtering and queries\n",
    "\n",
    "Designed for ESC-50 proof of concept but extensible to any IQ data (audio, RF signals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiftyOne version: 1.11.0\n",
      "Using device: cpu\n",
      "\n",
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, environment check, FiftyOne installation verification\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check and install FiftyOne if needed\n",
    "try:\n",
    "    import fiftyone as fo\n",
    "    import fiftyone.brain as fob\n",
    "    print(f\"FiftyOne version: {fo.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing FiftyOne...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fiftyone\", \"-q\"])\n",
    "    import fiftyone as fo\n",
    "    import fiftyone.brain as fob\n",
    "    print(f\"FiftyOne installed successfully. Version: {fo.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nEnvironment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audiomaepp.ipynb  data_loaders.py\t\tresearch.md\n",
      "audiomae.py\t  embeddings_utils.py\t\tspec.md\n",
      "checkpoints\t  fiftyone_visualization.ipynb\ttest_improvements.py\n",
      "CLAUDE.md\t  plan.md\n",
      "data\t\t  __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Checkpoint: checkpoints/encoder_only(4).pt\n",
      "  Data root: data/ESC-50-master\n",
      "  Cache dir: data/embeddings/esc50_audiomae\n",
      "  FiftyOne dataset: esc50_audiomae\n",
      "  Batch size: 4\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration class with all parameters\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for FiftyOne embedding visualization pipeline.\"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    checkpoint_path = Path(\"checkpoints/encoder_only(4).pt\")  # UPDATE THIS!\n",
    "    device = device\n",
    "    \n",
    "    # Data Paths\n",
    "    data_root = Path(\"data/ESC-50-master\")\n",
    "    audio_dir = data_root / \"audio\"\n",
    "    metadata_csv = data_root / \"meta\" / \"esc50.csv\"\n",
    "    spectrogram_dir = Path(\"data/imgs/full\")  # Use precomputed spectrograms for speed\n",
    "    \n",
    "    # Processing Configuration\n",
    "    batch_size = 4\n",
    "    num_workers = 0  # Set to 0 for Jupyter notebooks to avoid multiprocessing issues\n",
    "    use_cache = True\n",
    "    cache_dir = Path(\"data/embeddings/esc50_audiomae\")\n",
    "    \n",
    "    # FiftyOne Configuration\n",
    "    dataset_name = \"esc50_audiomae\"\n",
    "    persistent = True\n",
    "    embedding_field = \"audiomae_embedding\"\n",
    "    \n",
    "    # Visualization Configuration\n",
    "    compute_umap = True\n",
    "    compute_tsne = True\n",
    "    add_synthetic_geo = True  # Generate synthetic lat/long for PoC\n",
    "    \n",
    "    # Model Architecture (must match checkpoint)\n",
    "    img_size = 256\n",
    "    patch_size = 16\n",
    "    embed_dim = 768\n",
    "    encoder_depth = 12\n",
    "    encoder_heads = 12\n",
    "    decoder_embed_dim = 512\n",
    "    decoder_depth = 8\n",
    "    decoder_heads = 16\n",
    "    use_macaron = True\n",
    "    use_swiglu = True\n",
    "    use_rope = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Checkpoint: {config.checkpoint_path}\")\n",
    "print(f\"  Data root: {config.data_root}\")\n",
    "print(f\"  Cache dir: {config.cache_dir}\")\n",
    "print(f\"  FiftyOne dataset: {config.dataset_name}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path validation: All paths exist!\n",
      "\n",
      "Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Helper utilities (progress tracking, error handling, file validation)\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Track and display pipeline progress.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = {}\n",
    "        \n",
    "    def start(self, step_name):\n",
    "        \"\"\"Mark a step as started.\"\"\"\n",
    "        self.steps[step_name] = {\"status\": \"running\", \"start_time\": pd.Timestamp.now()}\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting: {step_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    def complete(self, step_name):\n",
    "        \"\"\"Mark a step as completed.\"\"\"\n",
    "        if step_name in self.steps:\n",
    "            elapsed = pd.Timestamp.now() - self.steps[step_name][\"start_time\"]\n",
    "            self.steps[step_name][\"status\"] = \"completed\"\n",
    "            self.steps[step_name][\"elapsed\"] = elapsed\n",
    "            print(f\"\\nCompleted: {step_name} (Elapsed: {elapsed.total_seconds():.2f}s)\")\n",
    "            \n",
    "    def summary(self):\n",
    "        \"\"\"Display summary of all steps.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Pipeline Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for step, info in self.steps.items():\n",
    "            status = info[\"status\"]\n",
    "            elapsed = info.get(\"elapsed\", \"N/A\")\n",
    "            if elapsed != \"N/A\":\n",
    "                elapsed = f\"{elapsed.total_seconds():.2f}s\"\n",
    "            print(f\"  {step}: {status} ({elapsed})\")\n",
    "\n",
    "def validate_paths(config):\n",
    "    \"\"\"Validate that all required paths exist.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    if not config.data_root.exists():\n",
    "        errors.append(f\"Data root not found: {config.data_root}\")\n",
    "    \n",
    "    if not config.audio_dir.exists():\n",
    "        errors.append(f\"Audio directory not found: {config.audio_dir}\")\n",
    "        \n",
    "    if not config.metadata_csv.exists():\n",
    "        errors.append(f\"Metadata CSV not found: {config.metadata_csv}\")\n",
    "    \n",
    "    if not config.checkpoint_path.exists():\n",
    "        errors.append(f\"Checkpoint not found: {config.checkpoint_path}\")\n",
    "        errors.append(\"  Please update config.checkpoint_path to point to your trained model.\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\nValidation Errors:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nPath validation: All paths exist!\")\n",
    "    return True\n",
    "\n",
    "# Initialize progress tracker\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "# Validate paths\n",
    "if validate_paths(config):\n",
    "    print(\"\\nReady to proceed!\")\n",
    "else:\n",
    "    print(\"\\nPlease fix the errors above before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Data Discovery\n",
      "============================================================\n",
      "Loaded metadata: 2000 rows\n",
      "\n",
      "Metadata columns: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take']\n",
      "\n",
      "First few rows:\n",
      "            filename  fold  target        category  esc10  src_file take\n",
      "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
      "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
      "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
      "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
      "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A\n",
      "\n",
      "Found 2000 audio files\n",
      "\n",
      "All 2000 audio files verified!\n",
      "\n",
      "Completed: Data Discovery (Elapsed: 0.51s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Discover audio files, load ESC-50 metadata CSV\n",
    "\n",
    "tracker.start(\"Data Discovery\")\n",
    "\n",
    "# Load metadata from CSV\n",
    "metadata_df = pd.read_csv(config.metadata_csv)\n",
    "print(f\"Loaded metadata: {len(metadata_df)} rows\")\n",
    "print(f\"\\nMetadata columns: {list(metadata_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(metadata_df.head())\n",
    "\n",
    "# Verify audio files exist\n",
    "audio_files = sorted(config.audio_dir.glob(\"*.wav\"))\n",
    "print(f\"\\nFound {len(audio_files)} audio files\")\n",
    "\n",
    "# Add full file paths to metadata\n",
    "metadata_df['filepath'] = metadata_df['filename'].apply(lambda x: str(config.audio_dir / x))\n",
    "\n",
    "# Verify all files exist\n",
    "missing_files = []\n",
    "for idx, row in metadata_df.iterrows():\n",
    "    if not Path(row['filepath']).exists():\n",
    "        missing_files.append(row['filename'])\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nWarning: {len(missing_files)} files missing from metadata\")\n",
    "    print(f\"First few missing: {missing_files[:5]}\")\n",
    "else:\n",
    "    print(f\"\\nAll {len(metadata_df)} audio files verified!\")\n",
    "\n",
    "tracker.complete(\"Data Discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Metadata Enrichment\n",
      "============================================================\n",
      "Added synthetic geographic coordinates:\n",
      "  Latitude range: [-1.63, 1.58]\n",
      "  Longitude range: [-1.05, 1.09]\n",
      "\n",
      "Coordinates clustered by category for meaningful visualization.\n",
      "\n",
      "Completed: Metadata Enrichment (Elapsed: 0.02s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Enrich metadata, add synthetic lat/long for PoC\n",
    "\n",
    "tracker.start(\"Metadata Enrichment\")\n",
    "\n",
    "def generate_synthetic_geo(category, seed=42):\n",
    "    \"\"\"Generate synthetic lat/long clustered by category.\n",
    "    \n",
    "    This creates geographic patterns where similar sound categories\n",
    "    are clustered in specific regions for visualization demo.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed + hash(category) % 1000)\n",
    "    \n",
    "    # Define category clusters (rough geographic regions)\n",
    "    category_centers = {\n",
    "        'Animals': (40.7128, -74.0060),      # New York area\n",
    "        'Natural soundscapes and water sounds': (47.6062, -122.3321),  # Seattle area\n",
    "        'Human, non-speech sounds': (51.5074, -0.1278),  # London area\n",
    "        'Interior/domestic sounds': (35.6762, 139.6503),  # Tokyo area\n",
    "        'Exterior/urban noises': (34.0522, -118.2437)  # Los Angeles area\n",
    "    }\n",
    "    \n",
    "    # Get center for this category\n",
    "    center_lat, center_lon = category_centers.get(category, (0.0, 0.0))\n",
    "    \n",
    "    # Add random offset (within ~50km radius)\n",
    "    lat_offset = np.random.normal(0, 0.5)\n",
    "    lon_offset = np.random.normal(0, 0.5)\n",
    "    \n",
    "    return center_lat + lat_offset, center_lon + lon_offset\n",
    "\n",
    "if config.add_synthetic_geo:\n",
    "    # Generate synthetic coordinates\n",
    "    coords = metadata_df['category'].apply(\n",
    "        lambda cat: generate_synthetic_geo(cat)\n",
    "    )\n",
    "    metadata_df['latitude'] = coords.apply(lambda x: x[0])\n",
    "    metadata_df['longitude'] = coords.apply(lambda x: x[1])\n",
    "    \n",
    "    print(\"Added synthetic geographic coordinates:\")\n",
    "    print(f\"  Latitude range: [{metadata_df['latitude'].min():.2f}, {metadata_df['latitude'].max():.2f}]\")\n",
    "    print(f\"  Longitude range: [{metadata_df['longitude'].min():.2f}, {metadata_df['longitude'].max():.2f}]\")\n",
    "    print(\"\\nCoordinates clustered by category for meaningful visualization.\")\n",
    "else:\n",
    "    print(\"Skipping synthetic geo generation (config.add_synthetic_geo = False)\")\n",
    "\n",
    "tracker.complete(\"Metadata Enrichment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Dataset Statistics\n",
      "============================================================\n",
      "ESC-50 Dataset Statistics:\n",
      "  Total samples: 2000\n",
      "  Number of classes: 50\n",
      "  Number of categories: 50\n",
      "  Number of folds: 5\n",
      "\n",
      "Category distribution:\n",
      "  dog: 40 samples\n",
      "  chirping_birds: 40 samples\n",
      "  vacuum_cleaner: 40 samples\n",
      "  thunderstorm: 40 samples\n",
      "  door_wood_knock: 40 samples\n",
      "  can_opening: 40 samples\n",
      "  crow: 40 samples\n",
      "  clapping: 40 samples\n",
      "  fireworks: 40 samples\n",
      "  chainsaw: 40 samples\n",
      "  airplane: 40 samples\n",
      "  mouse_click: 40 samples\n",
      "  pouring_water: 40 samples\n",
      "  train: 40 samples\n",
      "  sheep: 40 samples\n",
      "  water_drops: 40 samples\n",
      "  church_bells: 40 samples\n",
      "  clock_alarm: 40 samples\n",
      "  keyboard_typing: 40 samples\n",
      "  wind: 40 samples\n",
      "  footsteps: 40 samples\n",
      "  frog: 40 samples\n",
      "  cow: 40 samples\n",
      "  brushing_teeth: 40 samples\n",
      "  car_horn: 40 samples\n",
      "  crackling_fire: 40 samples\n",
      "  helicopter: 40 samples\n",
      "  drinking_sipping: 40 samples\n",
      "  rain: 40 samples\n",
      "  insects: 40 samples\n",
      "  laughing: 40 samples\n",
      "  hen: 40 samples\n",
      "  engine: 40 samples\n",
      "  breathing: 40 samples\n",
      "  crying_baby: 40 samples\n",
      "  hand_saw: 40 samples\n",
      "  coughing: 40 samples\n",
      "  glass_breaking: 40 samples\n",
      "  snoring: 40 samples\n",
      "  toilet_flush: 40 samples\n",
      "  pig: 40 samples\n",
      "  washing_machine: 40 samples\n",
      "  clock_tick: 40 samples\n",
      "  sneezing: 40 samples\n",
      "  rooster: 40 samples\n",
      "  sea_waves: 40 samples\n",
      "  siren: 40 samples\n",
      "  cat: 40 samples\n",
      "  door_wood_creaks: 40 samples\n",
      "  crickets: 40 samples\n",
      "\n",
      "Class distribution (top 10):\n",
      "  Class 0: 40 samples\n",
      "  Class 14: 40 samples\n",
      "  Class 36: 40 samples\n",
      "  Class 19: 40 samples\n",
      "  Class 30: 40 samples\n",
      "  Class 34: 40 samples\n",
      "  Class 9: 40 samples\n",
      "  Class 22: 40 samples\n",
      "  Class 48: 40 samples\n",
      "  Class 41: 40 samples\n",
      "\n",
      "Fold distribution:\n",
      "  Fold 1: 400 samples\n",
      "  Fold 2: 400 samples\n",
      "  Fold 3: 400 samples\n",
      "  Fold 4: 400 samples\n",
      "  Fold 5: 400 samples\n",
      "\n",
      "Completed: Dataset Statistics (Elapsed: 0.01s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Display dataset statistics\n",
    "\n",
    "tracker.start(\"Dataset Statistics\")\n",
    "\n",
    "print(\"ESC-50 Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(metadata_df)}\")\n",
    "print(f\"  Number of classes: {metadata_df['target'].nunique()}\")\n",
    "print(f\"  Number of categories: {metadata_df['category'].nunique()}\")\n",
    "print(f\"  Number of folds: {metadata_df['fold'].nunique()}\")\n",
    "\n",
    "print(\"\\nCategory distribution:\")\n",
    "category_counts = metadata_df['category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category}: {count} samples\")\n",
    "\n",
    "print(\"\\nClass distribution (top 10):\")\n",
    "class_counts = metadata_df['target'].value_counts().head(10)\n",
    "for target, count in class_counts.items():\n",
    "    print(f\"  Class {target}: {count} samples\")\n",
    "\n",
    "print(\"\\nFold distribution:\")\n",
    "fold_counts = metadata_df['fold'].value_counts().sort_index()\n",
    "for fold, count in fold_counts.items():\n",
    "    print(f\"  Fold {fold}: {count} samples\")\n",
    "\n",
    "tracker.complete(\"Dataset Statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Model Loading\n",
      "============================================================\n",
      "Created AudioMAE++ model\n",
      "[OK] Loaded full model checkpoint\n",
      "\n",
      "Checkpoint info: {'epoch': 'unknown', 'loss': 'unknown', 'has_optimizer': False}\n",
      "Model loaded successfully to cpu\n",
      "Model parameters: 221.87M\n",
      "\n",
      "Completed: Model Loading (Elapsed: 10.05s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load AudioMAE checkpoint from user path\n",
    "\n",
    "tracker.start(\"Model Loading\")\n",
    "\n",
    "# Import AudioMAE model and CheckpointLoader\n",
    "from audiomae import AudioMAEPlusPlus, Config as AudioMAEConfig\n",
    "from embeddings_utils import CheckpointLoader\n",
    "\n",
    "# Create model config matching checkpoint\n",
    "model_config = AudioMAEConfig()\n",
    "model_config.img_size = config.img_size\n",
    "model_config.patch_size = config.patch_size\n",
    "model_config.embed_dim = config.embed_dim\n",
    "model_config.encoder_depth = config.encoder_depth\n",
    "model_config.encoder_heads = config.encoder_heads\n",
    "model_config.decoder_embed_dim = config.decoder_embed_dim\n",
    "model_config.decoder_depth = config.decoder_depth\n",
    "model_config.decoder_heads = config.decoder_heads\n",
    "model_config.use_macaron = config.use_macaron\n",
    "model_config.use_swiglu = config.use_swiglu\n",
    "model_config.use_rope = config.use_rope\n",
    "\n",
    "# Create model\n",
    "model = AudioMAEPlusPlus(model_config)\n",
    "print(f\"Created AudioMAE++ model\")\n",
    "\n",
    "# Load checkpoint using CheckpointLoader (handles encoder-only automatically)\n",
    "try:\n",
    "    loader = CheckpointLoader(config.checkpoint_path, device=config.device)\n",
    "    model, checkpoint_info = loader.load(model)\n",
    "    \n",
    "    print(f\"\\nCheckpoint info: {checkpoint_info}\")\n",
    "    print(f\"Model loaded successfully to {config.device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading checkpoint: {e}\")\n",
    "    print(\"\\nPlease verify:\")\n",
    "    print(f\"  1. Checkpoint path is correct: {config.checkpoint_path}\")\n",
    "    print(f\"  2. Model config matches checkpoint (embed_dim={config.embed_dim}, etc.)\")\n",
    "    raise\n",
    "\n",
    "tracker.complete(\"Model Loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingGenerator class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define EmbeddingGenerator class\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for loading precomputed spectrograms.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_df, spectrogram_dir):\n",
    "        self.metadata = metadata_df.reset_index(drop=True)\n",
    "        self.spectrogram_dir = Path(spectrogram_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # Get spectrogram filename (same as audio filename but .npy)\n",
    "        audio_filename = row['filename']\n",
    "        spec_filename = audio_filename.replace('.wav', '.npy')\n",
    "        spec_path = self.spectrogram_dir / spec_filename\n",
    "        \n",
    "        # Load spectrogram\n",
    "        try:\n",
    "            spectrogram = np.load(spec_path)\n",
    "            spectrogram = torch.from_numpy(spectrogram).float()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Spectrogram not found: {spec_path}\")\n",
    "            # Return zeros if file not found\n",
    "            spectrogram = torch.zeros(3, 224, 224, dtype=torch.float32)\n",
    "        \n",
    "        return spectrogram, idx\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings from AudioMAE model with batch processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, batch_size=32):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def extract_cls_embedding(self, spectrograms):\n",
    "        \"\"\"Extract CLS token embedding from AudioMAE encoder.\n",
    "        \n",
    "        Args:\n",
    "            spectrograms: Tensor of shape [B, 3, 224, 224]\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: Tensor of shape [B, 768]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward encoder with NO masking (mask_ratio=0.0)\n",
    "            latent, _, _ = self.model.forward_encoder(spectrograms, mask_ratio=0.0)\n",
    "            # CLS token is at position 0: latent[:, 0, :]\n",
    "            cls_embedding = latent[:, 0, :].cpu().numpy()  # Shape: [B, 768]\n",
    "        return cls_embedding\n",
    "    \n",
    "    def generate_embeddings(self, dataset, desc=\"Generating embeddings\"):\n",
    "        \"\"\"Generate embeddings for entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset: PyTorch Dataset returning (spectrogram, index)\n",
    "            desc: Description for progress bar\n",
    "            \n",
    "        Returns:\n",
    "            embeddings: numpy array of shape [N, 768]\n",
    "            indices: numpy array of indices\n",
    "        \"\"\"\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Avoid multiprocessing in notebooks\n",
    "            pin_memory=True if self.device == \"cuda\" else False\n",
    "        )\n",
    "        \n",
    "        all_embeddings = []\n",
    "        all_indices = []\n",
    "        \n",
    "        for spectrograms, indices in tqdm(dataloader, desc=desc):\n",
    "            spectrograms = spectrograms.to(self.device)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings = self.extract_cls_embedding(spectrograms)\n",
    "            \n",
    "            all_embeddings.append(embeddings)\n",
    "            all_indices.append(indices.numpy())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        embeddings = np.vstack(all_embeddings)\n",
    "        indices = np.concatenate(all_indices)\n",
    "        \n",
    "        return embeddings, indices\n",
    "\n",
    "print(\"EmbeddingGenerator class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Embedding Generation\n",
      "============================================================\n",
      "Loading cached embeddings from data/embeddings/esc50_audiomae/embeddings.npy\n",
      "Loaded embeddings shape: (2000, 768)\n",
      "Cache metadata: {'shape': [2000, 768], 'num_samples': 2000, 'embedding_dim': 768, 'checkpoint': 'encoder_only.pt', 'dataset': 'esc50_audiomae', 'timestamp': '2025-12-29T20:10:35.046453'}\n",
      "\n",
      "Completed: Embedding Generation (Elapsed: 0.13s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Execute embedding generation on all 2000 samples\n",
    "\n",
    "tracker.start(\"Embedding Generation\")\n",
    "\n",
    "# Check if embeddings are cached\n",
    "cache_file = config.cache_dir / \"embeddings.npy\"\n",
    "metadata_cache_file = config.cache_dir / \"metadata.json\"\n",
    "\n",
    "if config.use_cache and cache_file.exists():\n",
    "    print(f\"Loading cached embeddings from {cache_file}\")\n",
    "    embeddings = np.load(cache_file)\n",
    "    print(f\"Loaded embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Load metadata to verify\n",
    "    if metadata_cache_file.exists():\n",
    "        with open(metadata_cache_file, 'r') as f:\n",
    "            cache_metadata = json.load(f)\n",
    "        print(f\"Cache metadata: {cache_metadata}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Generating embeddings from scratch...\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SpectrogramDataset(metadata_df, config.spectrogram_dir)\n",
    "    print(f\"Created dataset with {len(dataset)} samples\")\n",
    "    \n",
    "    # Create embedding generator\n",
    "    generator = EmbeddingGenerator(\n",
    "        model=model,\n",
    "        device=config.device,\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings, indices = generator.generate_embeddings(\n",
    "        dataset,\n",
    "        desc=\"Extracting embeddings\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Expected shape: ({len(metadata_df)}, {config.embed_dim})\")\n",
    "    \n",
    "    # Verify no NaNs\n",
    "    nan_count = np.isnan(embeddings).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"\\nWarning: Found {nan_count} NaN values in embeddings!\")\n",
    "    else:\n",
    "        print(\"\\nNo NaN values detected - embeddings are valid!\")\n",
    "\n",
    "# Add embeddings to metadata dataframe for easy access\n",
    "metadata_df['embedding_idx'] = range(len(metadata_df))\n",
    "\n",
    "tracker.complete(\"Embedding Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already cached or caching disabled.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Cache embeddings as .npy for reuse\n",
    "\n",
    "if config.use_cache and not cache_file.exists():\n",
    "    tracker.start(\"Caching Embeddings\")\n",
    "    \n",
    "    # Create cache directory\n",
    "    config.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save(cache_file, embeddings)\n",
    "    print(f\"Saved embeddings to {cache_file}\")\n",
    "    print(f\"File size: {cache_file.stat().st_size / 1e6:.2f} MB\")\n",
    "    \n",
    "    # Save metadata\n",
    "    cache_metadata = {\n",
    "        \"shape\": list(embeddings.shape),\n",
    "        \"num_samples\": len(metadata_df),\n",
    "        \"embedding_dim\": config.embed_dim,\n",
    "        \"checkpoint\": str(config.checkpoint_path),\n",
    "        \"dataset\": config.dataset_name,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(metadata_cache_file, 'w') as f:\n",
    "        json.dump(cache_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved metadata to {metadata_cache_file}\")\n",
    "    \n",
    "    # Save config for reproducibility\n",
    "    config_cache_file = config.cache_dir / \"config.json\"\n",
    "    config_dict = {\n",
    "        \"img_size\": config.img_size,\n",
    "        \"patch_size\": config.patch_size,\n",
    "        \"embed_dim\": config.embed_dim,\n",
    "        \"encoder_depth\": config.encoder_depth,\n",
    "        \"use_macaron\": config.use_macaron,\n",
    "        \"use_swiglu\": config.use_swiglu,\n",
    "        \"use_rope\": config.use_rope,\n",
    "    }\n",
    "    \n",
    "    with open(config_cache_file, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved config to {config_cache_file}\")\n",
    "    \n",
    "    tracker.complete(\"Caching Embeddings\")\n",
    "    \n",
    "else:\n",
    "    print(\"Embeddings already cached or caching disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: FiftyOne Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: FiftyOne Dataset Creation\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing dataset: esc50_audiomae\n",
      "Created FiftyOne dataset: esc50_audiomae\n",
      "Persistent: True\n",
      "\n",
      "Adding 2000 samples to dataset...\n",
      "Using PNG spectrograms as primary media for visual exploration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating samples: 100%|██████████| 2000/2000 [00:00<00:00, 4280.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2000/2000 [410.2ms elapsed, 0s remaining, 4.9K samples/s]     \n",
      "\n",
      "Dataset created with 2000 samples\n",
      "\n",
      "Primary media: PNG spectrograms from data/imgs/pre\n",
      "Audio files accessible via 'audio_filepath' field\n",
      "\n",
      "Completed: FiftyOne Dataset Creation (Elapsed: 9.50s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Create FiftyOne dataset, add samples with file paths\n",
    "\n",
    "tracker.start(\"FiftyOne Dataset Creation\")\n",
    "\n",
    "# Delete existing dataset if it exists\n",
    "if config.dataset_name in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {config.dataset_name}\")\n",
    "    fo.delete_dataset(config.dataset_name)\n",
    "\n",
    "# Create new dataset\n",
    "dataset = fo.Dataset(\n",
    "    name=config.dataset_name,\n",
    "    persistent=config.persistent\n",
    ")\n",
    "\n",
    "print(f\"Created FiftyOne dataset: {config.dataset_name}\")\n",
    "print(f\"Persistent: {config.persistent}\")\n",
    "\n",
    "# Add samples to dataset\n",
    "print(f\"\\nAdding {len(metadata_df)} samples to dataset...\")\n",
    "print(\"Using PNG spectrograms as primary media for visual exploration\")\n",
    "\n",
    "# Define paths for PNG spectrograms\n",
    "png_dir = Path(\"data/imgs/pre\")\n",
    "\n",
    "samples = []\n",
    "missing_pngs = []\n",
    "\n",
    "for idx, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Creating samples\"):\n",
    "    # Get PNG spectrogram path (primary media)\n",
    "    audio_filename = row['filename']\n",
    "    png_filename = audio_filename.replace('.wav', '.png')\n",
    "    png_path = png_dir / png_filename\n",
    "    \n",
    "    # Check if PNG exists\n",
    "    if not png_path.exists():\n",
    "        missing_pngs.append(png_filename)\n",
    "        continue\n",
    "    \n",
    "    # Create sample with PNG as primary filepath\n",
    "    sample = fo.Sample(filepath=str(png_path))\n",
    "    \n",
    "    # Add audio filepath as separate field for reference\n",
    "    sample[\"audio_filepath\"] = row['filepath']\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples)\n",
    "\n",
    "print(f\"\\nDataset created with {len(dataset)} samples\")\n",
    "if missing_pngs:\n",
    "    print(f\"Warning: {len(missing_pngs)} PNG files missing (samples skipped)\")\n",
    "    print(f\"First few missing: {missing_pngs[:5]}\")\n",
    "\n",
    "print(f\"\\nPrimary media: PNG spectrograms from {png_dir}\")\n",
    "print(f\"Audio files accessible via 'audio_filepath' field\")\n",
    "\n",
    "tracker.complete(\"FiftyOne Dataset Creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Adding Metadata Fields\n",
      "============================================================\n",
      "Adding metadata fields to FiftyOne samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding metadata: 100%|██████████| 2000/2000 [00:04<00:00, 468.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Added metadata to all 2000 samples\n",
      "\n",
      "Dataset fields: ['id', 'filepath', 'tags', 'metadata', 'created_at', 'last_modified_at', 'audio_filepath', 'label', 'category', 'fold', 'esc10', 'filename', 'location']\n",
      "\n",
      "Completed: Adding Metadata Fields (Elapsed: 4.28s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Add metadata fields (labels, fold, category, GeoLocation)\n",
    "\n",
    "tracker.start(\"Adding Metadata Fields\")\n",
    "\n",
    "print(\"Adding metadata fields to FiftyOne samples...\")\n",
    "\n",
    "for sample, (idx, row) in tqdm(\n",
    "    zip(dataset, metadata_df.iterrows()),\n",
    "    total=len(dataset),\n",
    "    desc=\"Adding metadata\"\n",
    "):\n",
    "    # Add label (class)\n",
    "    sample[\"label\"] = fo.Classification(label=str(row['target']))\n",
    "    \n",
    "    # Add category\n",
    "    sample[\"category\"] = row['category']\n",
    "    \n",
    "    # Add fold (for cross-validation)\n",
    "    sample[\"fold\"] = int(row['fold'])\n",
    "    \n",
    "    # Add ESC-10 indicator\n",
    "    sample[\"esc10\"] = bool(row['esc10'])\n",
    "    \n",
    "    # Add filename\n",
    "    sample[\"filename\"] = row['filename']\n",
    "    \n",
    "    # Add geographic location (if available)\n",
    "    if config.add_synthetic_geo and 'latitude' in row and 'longitude' in row:\n",
    "        # Note: FiftyOne expects [longitude, latitude] order!\n",
    "        sample[\"location\"] = fo.GeoLocation(\n",
    "            point=[row['longitude'], row['latitude']]\n",
    "        )\n",
    "    \n",
    "    sample.save()\n",
    "\n",
    "print(f\"\\nAdded metadata to all {len(dataset)} samples\")\n",
    "print(f\"\\nDataset fields: {list(dataset.get_field_schema().keys())}\")\n",
    "\n",
    "tracker.complete(\"Adding Metadata Fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Attaching Embeddings\n",
      "============================================================\n",
      "Attaching 768-dimensional embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching embeddings: 100%|██████████| 2000/2000 [00:09<00:00, 215.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attached embeddings to all 2000 samples\n",
      "Embedding field name: audiomae_embedding\n",
      "Embedding dimension: 768\n",
      "\n",
      "Example embedding shape: 768\n",
      "First 10 values: [-0.08253182470798492, 0.7688832879066467, -0.1343298703432083, -0.589703381061554, 0.7069021463394165, -0.6623245477676392, -0.3486599624156952, -0.19210349023342133, -0.336894154548645, 0.714085042476654]\n",
      "\n",
      "Completed: Attaching Embeddings (Elapsed: 9.30s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Attach 768-dim embeddings to each sample\n",
    "\n",
    "tracker.start(\"Attaching Embeddings\")\n",
    "\n",
    "print(f\"Attaching {config.embed_dim}-dimensional embeddings...\")\n",
    "\n",
    "for sample, embedding in tqdm(\n",
    "    zip(dataset, embeddings),\n",
    "    total=len(dataset),\n",
    "    desc=\"Attaching embeddings\"\n",
    "):\n",
    "    # Convert numpy array to list for FiftyOne storage\n",
    "    sample[config.embedding_field] = embedding.tolist()\n",
    "    sample.save()\n",
    "\n",
    "print(f\"\\nAttached embeddings to all {len(dataset)} samples\")\n",
    "print(f\"Embedding field name: {config.embedding_field}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "# Verify embeddings\n",
    "sample = dataset.first()\n",
    "print(f\"\\nExample embedding shape: {len(sample[config.embedding_field])}\")\n",
    "print(f\"First 10 values: {sample[config.embedding_field][:10]}\")\n",
    "\n",
    "tracker.complete(\"Attaching Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Computing Similarity Index\n",
      "============================================================\n",
      "Computing similarity index for nearest neighbor search...\n",
      "\n",
      "Similarity index computed successfully!\n",
      "Brain key: audiomae_similarity\n",
      "Backend: sklearn\n",
      "\n",
      "Testing similarity search...\n",
      "Found 10 similar samples\n",
      "\n",
      "Top 5 most similar samples:\n",
      "  1. 1-100032-A-0.png (Category: dog)\n",
      "  2. 3-148932-A-34.png (Category: can_opening)\n",
      "  3. 5-252248-A-34.png (Category: can_opening)\n",
      "  4. 5-204604-A-24.png (Category: coughing)\n",
      "  5. 3-147342-A-34.png (Category: can_opening)\n",
      "\n",
      "Completed: Computing Similarity Index (Elapsed: 1.26s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Compute similarity index\n",
    "\n",
    "tracker.start(\"Computing Similarity Index\")\n",
    "\n",
    "print(\"Computing similarity index for nearest neighbor search...\")\n",
    "\n",
    "# Compute similarity index using sklearn backend\n",
    "fob.compute_similarity(\n",
    "    dataset,\n",
    "    embeddings=config.embedding_field,\n",
    "    brain_key=\"audiomae_similarity\",\n",
    "    backend=\"sklearn\",  # Fast, in-memory backend\n",
    ")\n",
    "\n",
    "print(f\"\\nSimilarity index computed successfully!\")\n",
    "print(f\"Brain key: audiomae_similarity\")\n",
    "print(f\"Backend: sklearn\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nTesting similarity search...\")\n",
    "query_sample = dataset.first()\n",
    "similar_view = dataset.sort_by_similarity(\n",
    "    query_sample.id,  # Use sample ID, not the sample object\n",
    "    k=10,\n",
    "    brain_key=\"audiomae_similarity\"\n",
    ")\n",
    "\n",
    "print(f\"Found {len(similar_view)} similar samples\")\n",
    "print(\"\\nTop 5 most similar samples:\")\n",
    "for i, sample in enumerate(similar_view[:5]):\n",
    "    print(f\"  {i+1}. {sample.filename} (Category: {sample.category})\")\n",
    "\n",
    "tracker.complete(\"Computing Similarity Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Computing Visualizations\n",
      "============================================================\n",
      "Computing UMAP visualization...\n",
      "Generating visualization...\n",
      "UMAP(n_jobs=1, random_state=42, verbose=True)\n",
      "Fri Jan  2 16:05:14 2026 Construct fuzzy simplicial set\n",
      "Fri Jan  2 16:05:18 2026 Finding Nearest Neighbors\n",
      "Fri Jan  2 16:05:24 2026 Finished Nearest Neighbor Search\n",
      "Fri Jan  2 16:05:27 2026 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:   5%| ▌          26/500 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  18%| █▊         90/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  37%| ███▋       184/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  150  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  45%| ████▌      226/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  200  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  58%| █████▊     288/500 [00:02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  250  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  66%| ██████▌    330/500 [00:02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  300  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  75%| ███████▍   373/500 [00:02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  350  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  88%| ████████▊  438/500 [00:02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  400  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  96%| █████████▌ 478/500 [00:03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  450  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  2 16:05:30 2026 Finished embedding\n",
      "UMAP visualization computed successfully!\n",
      "\n",
      "Computing t-SNE visualization...\n",
      "(This may take a few minutes for 2000 samples)\n",
      "Generating visualization...\n",
      "[t-SNE] Computing 301 nearest neighbors...\n",
      "[t-SNE] Indexed 2000 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 2000 samples in 0.242s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 2000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 2000\n",
      "[t-SNE] Mean sigma: 0.893845\n",
      "[t-SNE] Computed conditional probabilities in 0.158s\n",
      "[t-SNE] Iteration 50: error = 58.0708084, gradient norm = 0.0056193 (50 iterations in 0.683s)\n",
      "[t-SNE] Iteration 100: error = 57.5564270, gradient norm = 0.0002185 (50 iterations in 0.505s)\n",
      "[t-SNE] Iteration 150: error = 57.5538216, gradient norm = 0.0024789 (50 iterations in 0.664s)\n",
      "[t-SNE] Iteration 200: error = 57.5374451, gradient norm = 0.0005048 (50 iterations in 0.509s)\n",
      "[t-SNE] Iteration 250: error = 57.5514221, gradient norm = 0.0001396 (50 iterations in 0.504s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 57.551422\n",
      "[t-SNE] Iteration 300: error = 0.8816583, gradient norm = 0.0047659 (50 iterations in 0.615s)\n",
      "[t-SNE] Iteration 350: error = 0.8156642, gradient norm = 0.0030596 (50 iterations in 0.638s)\n",
      "[t-SNE] Iteration 400: error = 0.7972035, gradient norm = 0.0017812 (50 iterations in 0.654s)\n",
      "[t-SNE] Iteration 450: error = 0.7918641, gradient norm = 0.0010907 (50 iterations in 0.649s)\n",
      "[t-SNE] Iteration 500: error = 0.7900164, gradient norm = 0.0005665 (50 iterations in 0.644s)\n",
      "[t-SNE] Iteration 550: error = 0.7893235, gradient norm = 0.0002532 (50 iterations in 0.637s)\n",
      "[t-SNE] Iteration 600: error = 0.7890399, gradient norm = 0.0002216 (50 iterations in 0.661s)\n",
      "[t-SNE] Iteration 650: error = 0.7887676, gradient norm = 0.0001866 (50 iterations in 0.643s)\n",
      "[t-SNE] Iteration 700: error = 0.7885672, gradient norm = 0.0001361 (50 iterations in 0.643s)\n",
      "[t-SNE] Iteration 750: error = 0.7885339, gradient norm = 0.0000914 (50 iterations in 0.614s)\n",
      "[t-SNE] Iteration 800: error = 0.7884609, gradient norm = 0.0000851 (50 iterations in 0.741s)\n",
      "[t-SNE] Iteration 850: error = 0.7883279, gradient norm = 0.0000972 (50 iterations in 0.645s)\n",
      "[t-SNE] Iteration 900: error = 0.7883083, gradient norm = 0.0000711 (50 iterations in 0.648s)\n",
      "[t-SNE] Iteration 950: error = 0.7882313, gradient norm = 0.0000693 (50 iterations in 0.621s)\n",
      "[t-SNE] Iteration 1000: error = 0.7881951, gradient norm = 0.0000579 (50 iterations in 0.619s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.788195\n",
      "t-SNE visualization computed successfully!\n",
      "\n",
      "All visualizations ready!\n",
      "\n",
      "Completed: Computing Visualizations (Elapsed: 42.39s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Compute UMAP and t-SNE visualizations\n",
    "\n",
    "tracker.start(\"Computing Visualizations\")\n",
    "\n",
    "# UMAP visualization (recommended - faster and better preservation)\n",
    "if config.compute_umap:\n",
    "    print(\"Computing UMAP visualization...\")\n",
    "    fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=config.embedding_field,\n",
    "        method=\"umap\",\n",
    "        num_dims=2,\n",
    "        brain_key=\"umap_viz\",\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "    print(\"UMAP visualization computed successfully!\")\n",
    "\n",
    "# t-SNE visualization (alternative - slower, good for local structure)\n",
    "if config.compute_tsne:\n",
    "    print(\"\\nComputing t-SNE visualization...\")\n",
    "    print(\"(This may take a few minutes for 2000 samples)\")\n",
    "    fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=config.embedding_field,\n",
    "        method=\"tsne\",\n",
    "        num_dims=2,\n",
    "        brain_key=\"tsne_viz\",\n",
    "        perplexity=100,\n",
    "        seed=42\n",
    "    )\n",
    "    print(\"t-SNE visualization computed successfully!\")\n",
    "\n",
    "print(\"\\nAll visualizations ready!\")\n",
    "\n",
    "tracker.complete(\"Computing Visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mapbox token configured in FiftyOne app_config.json\n",
      "  Token: pk.eyJ1IjoibG91aXNhY...\n",
      "  Config file: /home/louis/.fiftyone/app_config.json\n",
      "\n",
      "✓ Map view should be available in FiftyOne App!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15.5: Verify Mapbox Configuration\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check FiftyOne app config for Mapbox token\n",
    "app_config_path = Path.home() / \".fiftyone\" / \"app_config.json\"\n",
    "\n",
    "if app_config_path.exists():\n",
    "    with open(app_config_path, 'r') as f:\n",
    "        app_config = json.load(f)\n",
    "    \n",
    "    mapbox_token = app_config.get(\"plugins\", {}).get(\"map\", {}).get(\"mapboxAccessToken\")\n",
    "    \n",
    "    if mapbox_token:\n",
    "        print(\"✓ Mapbox token configured in FiftyOne app_config.json\")\n",
    "        print(f\"  Token: {mapbox_token[:20]}...\")\n",
    "        print(f\"  Config file: {app_config_path}\")\n",
    "        print(\"\\n✓ Map view should be available in FiftyOne App!\")\n",
    "    else:\n",
    "        print(\"❌ Mapbox token not found in app_config.json\")\n",
    "        print(f\"\\nTo enable map visualization:\")\n",
    "        print(f\"  1. Get a free token: https://account.mapbox.com/access-tokens/\")\n",
    "        print(f\"  2. Add to {app_config_path}\")\n",
    "        print(f'     {{\"plugins\": {{\"map\": {{\"mapboxAccessToken\": \"YOUR_TOKEN\"}}}}}}')\n",
    "else:\n",
    "    print(f\"⚠ FiftyOne app config not found at {app_config_path}\")\n",
    "    print(\"\\nConfig will be created automatically when needed.\")\n",
    "    print(\"To manually configure Mapbox, see:\")\n",
    "    print(\"https://docs.voxel51.com/user_guide/config.html#configuring-a-mapbox-token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting: Launching FiftyOne App\n",
      "============================================================\n",
      "Launching FiftyOne App...\n",
      "\n",
      "The app will open in your browser.\n",
      "\n",
      "Available features:\n",
      "  - Browse audio samples with metadata\n",
      "  - Search by similarity (select a sample, then 'Sort by similarity')\n",
      "  - View geographic distribution (map view)\n",
      "  - Explore UMAP/t-SNE embeddings (Embeddings panel)\n",
      "  - Filter by category, fold, esc10, etc.\n",
      "\n",
      "Press Ctrl+C in the terminal to stop the app when done.\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "\n",
      "Dataset: esc50_audiomae\n",
      "Total samples: 2000\n",
      "\n",
      "Session active at: http://localhost:5151\n",
      "Open this URL in your browser to view the FiftyOne App\n",
      "\n",
      "Completed: Launching FiftyOne App (Elapsed: 21.35s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Launch FiftyOne App, display dataset\n",
    "\n",
    "tracker.start(\"Launching FiftyOne App\")\n",
    "\n",
    "print(\"Launching FiftyOne App...\")\n",
    "print(\"\\nThe app will open in your browser.\")\n",
    "print(\"\\nAvailable features:\")\n",
    "print(\"  - Browse audio samples with metadata\")\n",
    "print(\"  - Search by similarity (select a sample, then 'Sort by similarity')\")\n",
    "print(\"  - View geographic distribution (map view)\")\n",
    "print(\"  - Explore UMAP/t-SNE embeddings (Embeddings panel)\")\n",
    "print(\"  - Filter by category, fold, esc10, etc.\")\n",
    "print(\"\\nPress Ctrl+C in the terminal to stop the app when done.\")\n",
    "\n",
    "# Launch the app (auto=False to avoid notebook display issues)\n",
    "session = fo.launch_app(dataset, auto=False)\n",
    "\n",
    "print(f\"\\nDataset: {config.dataset_name}\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"\\nSession active at: http://localhost:{session.server_port}\")\n",
    "print(f\"Open this URL in your browser to view the FiftyOne App\")\n",
    "\n",
    "tracker.complete(\"Launching FiftyOne App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Queries\n",
      "============================================================\n",
      "\n",
      "1. Filtering by category: 'Animals'\n",
      "   Found 0 animal sounds\n",
      "\n",
      "2. Filtering by fold: fold 1\n",
      "   Found 400 samples in fold 1\n",
      "\n",
      "3. Filtering ESC-10 subset\n",
      "   Found 400 samples in ESC-10 subset\n",
      "\n",
      "4. Similarity search: Find sounds similar to first sample\n",
      "   Query sample: 1-100032-A-0.png\n",
      "   Category: dog\n",
      "\n",
      "   Top 5 most similar samples:\n",
      "     1. 1-100032-A-0.png\n",
      "        Category: dog\n",
      "        Label: 0\n",
      "     2. 3-148932-A-34.png\n",
      "        Category: can_opening\n",
      "        Label: 34\n",
      "     3. 5-252248-A-34.png\n",
      "        Category: can_opening\n",
      "        Label: 34\n",
      "     4. 5-204604-A-24.png\n",
      "        Category: coughing\n",
      "        Label: 24\n",
      "     5. 3-147342-A-34.png\n",
      "        Category: can_opening\n",
      "        Label: 34\n",
      "\n",
      "5. Combined filter: Animals in fold 1\n",
      "   Found 0 samples\n",
      "\n",
      "6. Geographic filtering: Samples near New York\n",
      "   Samples in 'Animals' category (clustered near NY): 0\n",
      "\n",
      "============================================================\n",
      "Demo queries complete!\n",
      "\n",
      "Try these in the FiftyOne App:\n",
      "  - Click a sample, then use 'Sort by similarity'\n",
      "  - Use the Embeddings panel to explore UMAP/t-SNE\n",
      "  - Use the map view to see geographic distribution\n",
      "  - Use filters in the sidebar to explore subsets\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Demo queries (similarity search, geo filtering, metadata queries)\n",
    "\n",
    "print(\"Demo Queries\\n\" + \"=\"*60)\n",
    "\n",
    "# 1. Filter by category\n",
    "print(\"\\n1. Filtering by category: 'Animals'\")\n",
    "animals_view = dataset.match({\"category\": \"Animals\"})\n",
    "print(f\"   Found {len(animals_view)} animal sounds\")\n",
    "\n",
    "# 2. Filter by fold (for cross-validation)\n",
    "print(\"\\n2. Filtering by fold: fold 1\")\n",
    "fold1_view = dataset.match({\"fold\": 1})\n",
    "print(f\"   Found {len(fold1_view)} samples in fold 1\")\n",
    "\n",
    "# 3. ESC-10 subset\n",
    "print(\"\\n3. Filtering ESC-10 subset\")\n",
    "esc10_view = dataset.match({\"esc10\": True})\n",
    "print(f\"   Found {len(esc10_view)} samples in ESC-10 subset\")\n",
    "\n",
    "# 4. Similarity search example\n",
    "print(\"\\n4. Similarity search: Find sounds similar to first sample\")\n",
    "query_sample = dataset.first()\n",
    "print(f\"   Query sample: {query_sample.filename}\")\n",
    "print(f\"   Category: {query_sample.category}\")\n",
    "\n",
    "similar_view = dataset.sort_by_similarity(\n",
    "    query_sample.id,  # Use sample ID, not the sample object\n",
    "    k=10,\n",
    "    brain_key=\"audiomae_similarity\"\n",
    ")\n",
    "\n",
    "print(f\"\\n   Top 5 most similar samples:\")\n",
    "for i, sample in enumerate(similar_view[:5]):\n",
    "    print(f\"     {i+1}. {sample.filename}\")\n",
    "    print(f\"        Category: {sample.category}\")\n",
    "    print(f\"        Label: {sample.label.label}\")\n",
    "\n",
    "# 5. Combined filter: Animals in fold 1\n",
    "print(\"\\n5. Combined filter: Animals in fold 1\")\n",
    "combined_view = dataset.match({\n",
    "    \"category\": \"Animals\",\n",
    "    \"fold\": 1\n",
    "})\n",
    "print(f\"   Found {len(combined_view)} samples\")\n",
    "\n",
    "# 6. Geographic filtering example (if synthetic geo was added)\n",
    "if config.add_synthetic_geo:\n",
    "    print(\"\\n6. Geographic filtering: Samples near New York\")\n",
    "    # Note: This is just a demo - real geographic queries would use FiftyOne's geo operators\n",
    "    ny_region = dataset.match({\"category\": \"Animals\"})  # Animals cluster near NY\n",
    "    print(f\"   Samples in 'Animals' category (clustered near NY): {len(ny_region)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Demo queries complete!\")\n",
    "print(\"\\nTry these in the FiftyOne App:\")\n",
    "print(\"  - Click a sample, then use 'Sort by similarity'\")\n",
    "print(\"  - Use the Embeddings panel to explore UMAP/t-SNE\")\n",
    "print(\"  - Use the map view to see geographic distribution\")\n",
    "print(\"  - Use filters in the sidebar to explore subsets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Options\n",
      "============================================================\n",
      "\n",
      "The FiftyOne dataset is already persistent and saved to disk.\n",
      "Dataset name: esc50_audiomae\n",
      "\n",
      "To reload this dataset in a future session:\n",
      "  import fiftyone as fo\n",
      "  dataset = fo.load_dataset('esc50_audiomae')\n",
      "  session = fo.launch_app(dataset)\n",
      "\n",
      "Embeddings are cached at:\n",
      "  data/embeddings/esc50_audiomae/embeddings.npy\n",
      "\n",
      "To export metadata to CSV:\n",
      "  metadata_df.to_csv('data/embeddings/esc50_audiomae/dataset_with_embeddings.csv', index=False)\n",
      "\n",
      "Dataset Information:\n",
      "{'samples_count': 2000, 'samples_bytes': 20932698, 'samples_size': '20.0MB', 'total_bytes': 20932698, 'total_size': '20.0MB'}\n",
      "\n",
      "============================================================\n",
      "Pipeline Summary\n",
      "============================================================\n",
      "  Data Discovery: completed (0.51s)\n",
      "  Metadata Enrichment: completed (0.02s)\n",
      "  Dataset Statistics: completed (0.01s)\n",
      "  Model Loading: completed (10.05s)\n",
      "  Embedding Generation: completed (0.13s)\n",
      "  FiftyOne Dataset Creation: completed (9.50s)\n",
      "  Adding Metadata Fields: completed (4.28s)\n",
      "  Attaching Embeddings: completed (9.30s)\n",
      "  Computing Similarity Index: completed (1.26s)\n",
      "  Computing Visualizations: completed (42.39s)\n",
      "  Launching FiftyOne App: completed (21.35s)\n",
      "\n",
      "============================================================\n",
      "FiftyOne Embedding Visualization System Ready!\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Explore the dataset in the FiftyOne App\n",
      "  2. Test similarity search on different audio types\n",
      "  3. Analyze UMAP/t-SNE clustering patterns\n",
      "  4. Filter and query by metadata fields\n",
      "  5. Extend to custom audio datasets using the same pipeline\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Export options (save dataset, export visualizations)\n",
    "\n",
    "print(\"Export Options\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nThe FiftyOne dataset is already persistent and saved to disk.\")\n",
    "print(f\"Dataset name: {config.dataset_name}\")\n",
    "\n",
    "# Show how to reload dataset in future sessions\n",
    "print(\"\\nTo reload this dataset in a future session:\")\n",
    "print(f\"  import fiftyone as fo\")\n",
    "print(f\"  dataset = fo.load_dataset('{config.dataset_name}')\")\n",
    "print(f\"  session = fo.launch_app(dataset)\")\n",
    "\n",
    "# Export embeddings\n",
    "print(\"\\nEmbeddings are cached at:\")\n",
    "print(f\"  {config.cache_dir / 'embeddings.npy'}\")\n",
    "\n",
    "# Export metadata\n",
    "print(\"\\nTo export metadata to CSV:\")\n",
    "export_csv = config.cache_dir / \"dataset_with_embeddings.csv\"\n",
    "print(f\"  metadata_df.to_csv('{export_csv}', index=False)\")\n",
    "\n",
    "# Show dataset info\n",
    "print(\"\\nDataset Information:\")\n",
    "print(dataset.stats())\n",
    "\n",
    "# Summary\n",
    "tracker.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FiftyOne Embedding Visualization System Ready!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Explore the dataset in the FiftyOne App\")\n",
    "print(\"  2. Test similarity search on different audio types\")\n",
    "print(\"  3. Analyze UMAP/t-SNE clustering patterns\")\n",
    "print(\"  4. Filter and query by metadata fields\")\n",
    "print(\"  5. Extend to custom audio datasets using the same pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fifty-one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
