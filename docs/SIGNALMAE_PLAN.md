# SignalMAE Implementation Plan

## Research Summary

### Approaches Reviewed

1. **[RIS-MAE](https://arxiv.org/abs/2508.00274)**: Raw IQ Signal MAE
   - Works directly on IQ time series
   - Random masking of IQ segments
   - Preserves modulation features better than spectrogram approaches
   - Good for few-shot and cross-domain transfer

2. **[GAF-MAE](https://www.researchgate.net/publication/374130230)**: Gramian Angular Field MAE
   - Converts IQ to Gramian Angular Fields
   - More complex preprocessing
   - Good for visualization

3. **[MSM-MAE](https://github.com/nttcslab/msm-mae)**: Masked Spectrogram Modeling
   - Works on spectrograms (similar to AudioMAE)
   - ViT backbone with patch masking
   - Strong benchmark results on audio tasks

4. **[FreqMAE](https://dl.acm.org/doi/10.1145/3589334.3645346)**: Frequency-Aware MAE
   - Physics-informed masking
   - Multi-modal IoT sensing focus

### Decision: Spectrogram-Based MAE (Simplest Approach)

**Rationale**:
- We already have TorchSig generating spectrograms (.npy and .png)
- AudioMAE++ architecture already exists and is well-tested
- Minimal new code needed - just adapt for RF spectrograms
- Can easily compare with audio results
- Future work can explore raw IQ approaches

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    SignalMAE (Baseline)                      │
├─────────────────────────────────────────────────────────────┤
│  Inherits from AudioMAE++ with RF-specific defaults          │
│                                                              │
│  Input: RF Spectrogram [B, 3, 224, 224]                     │
│         (Generated by TorchSig → IQToSpectrogram)           │
│                                                              │
│  Architecture:                                               │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Patch Embedding: 16x16 patches → 196 tokens          │   │
│  │ + CLS Token: → 197 tokens                            │   │
│  │ + Position Embeddings (learned)                      │   │
│  └──────────────────────────────────────────────────────┘   │
│                         ↓                                    │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Encoder: 6-12 Standard Transformer Blocks            │   │
│  │ (Simpler than AudioMAE++ - no Macaron/SwiGLU/RoPE)  │   │
│  └──────────────────────────────────────────────────────┘   │
│                         ↓                                    │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Decoder: 4-8 Standard Transformer Blocks             │   │
│  │ Reconstructs masked patches                          │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                              │
│  Output: Reconstruction loss + embeddings [B, 768]          │
└─────────────────────────────────────────────────────────────┘
```

## Implementation Plan

### Phase 1: Core Model (This PR)

#### 1.1 SignalMAE Model (`src/models/signalmae.py`)
- Subclass or wrap existing `BaselineMAE` (already exists!)
- Register as `@model_registry.register("signalmae")`
- RF-specific defaults (no advanced features for simplicity)
- Same input/output interface as AudioMAE++

```python
@model_registry.register("signalmae", version="1.0")
class SignalMAE(BaselineMAE):
    """
    Masked Autoencoder for RF signal spectrograms.

    Uses standard ViT-MAE architecture (no Macaron/SwiGLU/RoPE)
    for simplicity and as a baseline for RF signal processing.
    """
```

#### 1.2 RF Configuration (`src/config.py`)
- Add factory function `create_rf_config()`
- RF-specific presets (small, base, large)

```python
def create_rf_config(size: str = "base") -> Config:
    """Create configuration optimized for RF spectrograms."""
    sizes = {
        "small": Config(embed_dim=384, encoder_depth=6, decoder_depth=4, ...),
        "base": Config(embed_dim=768, encoder_depth=12, decoder_depth=8, ...),
    }
    return sizes[size]
```

#### 1.3 SignalMAE Classifier (`src/models/signalmae.py`)
- Reuse `AudioMAEClassifier` or create thin wrapper
- Fine-tuning for modulation classification

#### 1.4 Training Pipeline (`src/training/rf_trainer.py`)
- Simple training loop for pre-training
- Uses TorchSig data loader
- Checkpointing and logging

### Phase 2: Testing

#### 2.1 Unit Tests (`tests/test_signalmae.py`)
- Model instantiation and forward pass
- Input/output shape validation
- Masking behavior
- Embedding extraction
- Classifier wrapper

#### 2.2 Integration Tests
- End-to-end: Generate data → Train → Evaluate
- TorchSig data loader integration
- Checkpoint save/load

### Phase 3: Future Work (Not This PR)

- Raw IQ input model (RIS-MAE style)
- Advanced features (Macaron, SwiGLU, RoPE)
- Multi-task learning (detection + classification)
- Cross-domain transfer experiments

## File Structure

```
src/
├── models/
│   ├── signalmae.py          # NEW: SignalMAE model + classifier
│   └── ...
├── config.py                  # MODIFY: Add create_rf_config()
├── training/
│   ├── rf_trainer.py         # NEW: RF training pipeline
│   └── losses.py             # EXISTING: Reuse
└── ...

tests/
├── test_signalmae.py         # NEW: Comprehensive tests
└── ...

scripts/
├── train_signalmae.py        # NEW: CLI training script
└── ...
```

## Key Design Decisions

### 1. Simplicity First
- Use existing `BaselineMAE` as parent class
- No new architectural innovations in v1.0
- Standard ViT-MAE with learned position embeddings
- Focus on getting pipeline working end-to-end

### 2. Reuse Over Rewrite
- Inherit from `BaselineMAE` (already disables advanced features)
- Reuse `AudioMAEClassifier` for fine-tuning
- Reuse existing loss functions
- Reuse TorchSig data loaders

### 3. Same Input Format
- Spectrograms [B, 3, 224, 224] (same as AudioMAE++)
- TorchSig already generates compatible spectrograms
- No changes to transform pipeline needed

### 4. Testability
- Comprehensive unit tests
- Integration tests with synthetic data
- Clear separation of concerns

## Success Criteria

1. **Model Works**: SignalMAE can be instantiated and run forward pass
2. **Training Works**: Can pretrain on TorchSig-generated data
3. **Classification Works**: Can fine-tune for modulation classification
4. **Tests Pass**: All unit and integration tests pass
5. **Documentation**: Clear usage examples

## Timeline Estimate

- Core model: 1-2 hours
- Training pipeline: 1-2 hours
- Tests: 1-2 hours
- Documentation: 30 min
- Total: ~5-6 hours

## References

- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
- [AudioMAE Paper](https://arxiv.org/abs/2203.16691)
- [MSM-MAE GitHub](https://github.com/nttcslab/msm-mae)
- [TorchSig Documentation](https://torchsig.readthedocs.io/)
